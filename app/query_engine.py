"""
Query engine module for querying indexed code repositories.
"""
from typing import Optional, Dict, Any, List
from llama_index.core import VectorStoreIndex
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.response_synthesizers import get_response_synthesizer
from llama_index.core import Settings as LlamaSettings
from llama_index.llms.ollama import Ollama

from app.config import settings
from app.indexer import CodeIndexer
from app.query_classifier import QueryClassifier, QueryMetadata, QueryType
from app.graph_search import GraphSearchEngine
from app.graph_storage import GraphStorage
from app.llm_provider import get_llm_provider


class CodeQueryEngine:
    """Query engine for code repositories."""
    
    def __init__(
        self, 
        collection_name: str = "code_repository",
        use_ollama: bool = False
    ):
        """
        Initialize the query engine.
        
        Args:
            collection_name: Name of the ChromaDB collection
            use_ollama: Whether to use Ollama for LLM (optional)
        """
        self.collection_name = collection_name
        self.indexer = CodeIndexer()
        self.query_classifier = QueryClassifier()
        
        # Set up LLM Provider
        try:
            self.llm_provider = get_llm_provider()
            print(f"‚úÖ LLM Provider inicializado: {settings.LLM_PROVIDER}")
        except Exception as e:
            print(f"‚ö†Ô∏è  LLM Provider n√£o dispon√≠vel: {e}")
            self.llm_provider = None
        
        # Set up Graph Search
        try:
            self.graph_storage = GraphStorage(db_path=str(settings.GRAPH_DB_PATH))
            self.graph_search = GraphSearchEngine(
                graph_storage=self.graph_storage
            )
        except Exception as e:
            print(f"‚ö†Ô∏è  Graph Search n√£o dispon√≠vel: {e}")
            self.graph_storage = None
            self.graph_search = None
        
        # Set up LLM if requested
        if use_ollama:
            self.llm = Ollama(
                model=settings.OLLAMA_MODEL,
                base_url=settings.OLLAMA_BASE_URL
            )
            LlamaSettings.llm = self.llm
        else:
            # Just retrieve context without LLM
            self.llm = None
        
        # Load the index
        try:
            self.index = self.indexer.load_index(collection_name)
            print(f"‚úÖ √çndice carregado da cole√ß√£o: {collection_name}")
        except Exception as e:
            raise ValueError(
                f"N√£o foi poss√≠vel carregar o √≠ndice '{collection_name}'. "
                f"Por favor, indexe um reposit√≥rio primeiro. Erro: {e}"
            )
    
    def query(
        self,
        query: str,
        similarity_top_k: int = 5,
        return_context_only: bool = True,
        show_classifier_info: bool = True
    ) -> Dict[str, Any]:
        """
        Executa consulta com roteamento inteligente (semantic/graph/hybrid).
        
        Args:
            query: Consulta do usu√°rio
            similarity_top_k: N√∫mero de chunks sem√¢nticos a recuperar
            return_context_only: Se False, gera resposta com LLM
            show_classifier_info: Se True, inclui info de classifica√ß√£o
        
        Returns:
            Dict com resultados e metadados
        """
        # Etapa 1: Classificar query
        query_type = self.query_classifier.classify(query)
        
        result = {
            "query": query,
            "query_classification": None,
            "semantic_results": None,
            "semantic_count": None,
            "graph_results": None,
            "graph_count": None,
            "graph_type": None,
            "response": None
        }
        
        if show_classifier_info:
            result["query_classification"] = {
                "tipo": query_type.value,
                "estrategia": "Vector Search" if query_type == QueryType.SEMANTIC else (
                    "Graph Search" if query_type == QueryType.GRAPH else "Vector + Graph"
                )
            }
        
        # Etapa 2: Executar busca apropriada
        if query_type in [QueryType.SEMANTIC, QueryType.HYBRID]:
            semantic_result = self._semantic_search(
                query=query,
                similarity_top_k=similarity_top_k
            )
            result["semantic_results"] = semantic_result.get("context") or []
            result["semantic_count"] = len(result["semantic_results"])
        
        if query_type in [QueryType.GRAPH, QueryType.HYBRID]:
            graph_result = self._graph_search(query)
            result["graph_results"] = graph_result.get("results") or []
            result["graph_count"] = len(result["graph_results"])
            result["graph_type"] = graph_result.get("type")
        
        # Etapa 3: Gerar resposta com LLM se solicitado
        if not return_context_only:
            if self.llm_provider is None:
                result["response"] = "LLM n√£o dispon√≠vel. Configure GEMINI_API_KEY ou instale Ollama."
            else:
                try:
                    print(f"üîç DEBUG: return_context_only={return_context_only}, llm_provider={self.llm_provider}")
                    
                    context_text = self._build_context_for_llm(result)
                    print(f"üîç DEBUG: Context length={len(context_text)}")
                    
                    if context_text.strip():
                        print(f"üîç DEBUG: Chamando LLM...")
                        prompt = self._build_prompt(context_text, query)
                        llm_response = self.llm_provider.generate(prompt)
                        print(f"üîç DEBUG: LLM response length={len(llm_response)}")
                        result["response"] = llm_response
                    else:
                        result["response"] = "Nenhum contexto relevante encontrado para responder √† pergunta."
                except Exception as e:
                    print(f"‚ùå Erro ao gerar resposta LLM: {e}")
                    import traceback
                    traceback.print_exc()
                    result["response"] = f"Erro ao gerar resposta: {str(e)}"
        
        return result
    
    def _semantic_search(
        self,
        query: str,
        similarity_top_k: int,
        min_score: float = 0.0,
        max_context_chars: Optional[int] = None
    ) -> Dict[str, Any]:
        """Execute Vector Search and return context."""
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=similarity_top_k
        )
        
        nodes = retriever.retrieve(query)
        
        context = []
        total_chars = 0
        
        for i, node in enumerate(nodes):
            # Filtrar por score m√≠nimo
            if node.score < min_score:
                continue
            
            # Verificar limite de caracteres
            if max_context_chars is not None:
                chunk_size = len(node.text)
                if total_chars + chunk_size > max_context_chars:
                    break
                total_chars += chunk_size
            
            context.append({
                'rank': len(context) + 1,
                'file_path': node.metadata.get('file_path', 'unknown'),
                'file_name': node.metadata.get('file_name', 'unknown'),
                'file_type': node.metadata.get('file_type', 'unknown'),
                'score': node.score,
                'text': node.text
            })
        
        return {
            'context': context,
            'count': len(context)
        }
    
    def _graph_search(self, query: str) -> Dict[str, Any]:
        """Execute Graph Search e retorna rela√ß√µes de c√≥digo."""
        if not self.graph_search:
            return {'results': None, 'count': None, 'type': None}
        
        # Detectar tipo de query de grafo
        query_lower = query.lower()
        
        # Extrair nome da fun√ß√£o usando padr√µes melhorados
        import re
        
        func_name = self._extract_function_name(query)
        
        # Se n√£o conseguiu extrair nome de fun√ß√£o expl√≠cito, retornar None
        # (Graph Search requer nome de fun√ß√£o espec√≠fico)
        if not func_name or len(func_name) < 2:
            return {'results': None, 'count': None, 'type': None}
        
        # Detectar tipo de query baseado em palavras-chave
        if "quem chama" in query_lower or "chamado por" in query_lower or "chamadores" in query_lower:
            # "Quem chama compress_pdf?" ‚Üí find_callers
            results = self.graph_search.find_callers(func_name)
            result_type = "callers"
            
        elif "chama quais" in query_lower or "chama qual" in query_lower or "chama o que" in query_lower or "o que chama" in query_lower:
            # "compress_pdf chama quais fun√ß√µes?" ‚Üí find_calls
            results = self.graph_search.find_calls(func_name)
            result_type = "calls"
            
        elif "cadeia" in query_lower or "fluxo" in query_lower or "chain" in query_lower:
            results = self.graph_search.find_call_chain(func_name)
            result_type = "chain"
            
        else:
            # An√°lise de impacto por padr√£o
            results = self.graph_search.get_impact_analysis(func_name)
            result_type = "impact"
        
        return {
            'results': results,
            'count': len(results) if results else 0,
            'type': result_type
        }
    
    def _extract_function_name(self, query: str) -> str:
        """
        Extrai nome da fun√ß√£o de uma query.
        
        Exemplos:
        - "Quem chama compress_pdf?" ‚Üí "compress_pdf"
        - "compress_pdf chama quais fun√ß√µes?" ‚Üí "compress_pdf"
        - "O que compress_pdf chama?" ‚Üí "compress_pdf"
        """
        import re
        
        # Padr√£o 1: Procurar por identificadores v√°lidos em Python (func_name, func_name())
        pattern_with_parens = r'(\w+)\s*\(\s*\)'
        match = re.search(pattern_with_parens, query)
        if match:
            return match.group(1)
        
        # Padr√£o 2: Fun√ß√£o DEPOIS de keywords ("quem chama compress_pdf")
        # ‚ö†Ô∏è Este deve vir ANTES do padr√£o "fun√ß√£o ANTES de keyword"
        after_patterns = [
            r'chama\s+(?:a\s+)?(?:fun√ß√£o\s+)?(\w+)',     # "quem chama compress_pdf" ou "quem chama a fun√ß√£o compress_pdf"
            r'invoca\s+(?:a\s+)?(?:fun√ß√£o\s+)?(\w+)',
            r'usa\s+(?:a\s+)?(?:fun√ß√£o\s+)?(\w+)',
            r'depende\s+de\s+(\w+)',
        ]
        
        for pattern in after_patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                candidate = match.group(1)
                # Verificar se n√£o √© palavra comum
                if candidate.lower() not in ['fun√ß√£o', 'fun√ß√µes', 'qual', 'quais', 'que', 'o', 'a']:
                    return candidate
        
        # Padr√£o 3: Fun√ß√£o ANTES de keywords ("compress_pdf chama...")
        before_patterns = [
            r'(\w+)\s+chama\s+(?:quais|qual|o\s+que)',  # "compress_pdf chama quais"
            r'(\w+)\s+invoca',
            r'(\w+)\s+usa',
            r'(\w+)\s+depende',
        ]
        
        for pattern in before_patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                candidate = match.group(1)
                # Verificar se n√£o √© palavra comum/keyword
                if candidate.lower() not in ['quem', 'o', 'que', 'qual', 'quais', 'fun√ß√£o', 'fun√ß√µes']:
                    return candidate
        
        # Padr√£o 4: Identificador seguido de '?' ou fim de string
        pattern_word = r'(\w+)\s*\)?(\s*\?)?$'
        match = re.search(pattern_word, query)
        if match:
            candidate = match.group(1)
            # Evitar pegar palavras comuns
            if candidate.lower() not in ['fun√ß√£o', 'fun√ß√µes', 'qual', 'quais', 'que']:
                return candidate
        
        # Fallback: pegar palavra que parece um identificador (n√£o palavra comum)
        words = query.replace("?", "").replace("(", "").replace(")", "").split()
        common_words = {'quem', 'o', 'que', 'qual', 'quais', 'fun√ß√£o', 'fun√ß√µes', 'chama', 'a', 'de'}
        
        for word in reversed(words):
            if word and word[0].isalpha() and word.lower() not in common_words:
                return word
        
        return ""

    def _build_context_for_llm(self, result: Dict[str, Any]) -> str:
        """
        Constr√≥i contexto formatado para o LLM a partir dos resultados.
        
        Prioriza resultados de grafo (precisos) sobre resultados sem√¢nticos.
        
        Args:
            result: Dicion√°rio com semantic_results, graph_results, etc.
            
        Returns:
            String formatada com o contexto
        """
        context_parts = []
        
        # Extrair nome da fun√ß√£o da query original (se existir)
        query_str = result.get("query", "")
        func_name = self._extract_function_name(query_str) if query_str else ""
        
        # 1. Adicionar resultados de grafo primeiro (mais precisos)
        if result.get("graph_results") and result.get("graph_count", 0) > 0:
            context_parts.append("### AN√ÅLISE DE RELA√á√ïES DE C√ìDIGO\n")
            
            graph_type = result.get("graph_type", "unknown")
            graph_results = result["graph_results"]
            
            if graph_type == "callers":
                if func_name:
                    context_parts.append(f"**Fun√ß√µes que CHAMAM `{func_name}()`:**\n")
                else:
                    context_parts.append("**Fun√ß√µes que CHAMAM a fun√ß√£o consultada:**\n")
            elif graph_type == "calls":
                if func_name:
                    context_parts.append(f"**Fun√ß√µes CHAMADAS por `{func_name}()`:**\n")
                else:
                    context_parts.append("**Fun√ß√µes CHAMADAS pela fun√ß√£o consultada:**\n")
            elif graph_type == "chain":
                context_parts.append("**Cadeia de chamadas:**\n")
            else:
                context_parts.append("**An√°lise de impacto:**\n")
            
            for idx, item in enumerate(graph_results, 1):
                if isinstance(item, dict):
                    name = item.get("name", "unknown")
                    node_type = item.get("type", "function")
                    file_path = item.get("file_path", "")
                    context_parts.append(f"{idx}. `{name}` ({node_type}) - {file_path}\n")
                else:
                    context_parts.append(f"{idx}. {item}\n")
            
            context_parts.append("\n")
        
        # 2. Adicionar resultados sem√¢nticos (contexto de c√≥digo)
        if result.get("semantic_results") and result.get("semantic_count", 0) > 0:
            context_parts.append("### TRECHOS DE C√ìDIGO RELEVANTES\n\n")
            
            for idx, item in enumerate(result["semantic_results"], 1):
                file_path = item.get("file_path", "unknown")
                text = item.get("text", "")
                score = item.get("score", 0.0)
                
                context_parts.append(f"**[{idx}] {file_path}** (relev√¢ncia: {score:.2f})\n")
                context_parts.append("```python\n")
                context_parts.append(text)
                context_parts.append("\n```\n\n")
        
        return "".join(context_parts)

    def _build_prompt(self, context: str, query: str) -> str:
        """
        Constr√≥i prompt estruturado para o LLM.
        
        Args:
            context: Contexto formatado (c√≥digo + rela√ß√µes)
            query: Pergunta do usu√°rio
            
        Returns:
            Prompt completo para o LLM
        """
        prompt = f"""Voc√™ √© um assistente especializado em an√°lise de c√≥digo Python.

**CONTEXTO RECUPERADO:**
{context}

**PERGUNTA DO USU√ÅRIO:**
{query}

**INSTRU√á√ïES:**
1. Use APENAS as informa√ß√µes fornecidas acima no contexto
2. Se a resposta n√£o estiver no contexto, diga "N√£o tenho informa√ß√µes suficientes"
3. Cite especificamente fun√ß√µes, arquivos e relacionamentos mencionados
4. Seja preciso e objetivo
5. Use linguagem t√©cnica apropriada

**RESPOSTA:**"""
        
        return prompt

    def retrieve_context(
        self,
        query: str,
        similarity_top_k: int = 5,
        min_score: float = 0.0
    ) -> Dict[str, Any]:
        """
        Recupera apenas contexto de c√≥digo relevante (sem classifica√ß√£o/an√°lise).
        
        √ötil para:
        - Integra√ß√£o com LLMs externos
        - Debug e visualiza√ß√£o
        - Uso direto do contexto em outras aplica√ß√µes
        
        Args:
            query: Consulta do usu√°rio
            similarity_top_k: N√∫mero de chunks a recuperar
            min_score: Score m√≠nimo de relev√¢ncia (0.0-1.0)
            
        Returns:
            Dict com contexto recuperado e metadados
        """
        return self._semantic_search(
            query=query,
            similarity_top_k=similarity_top_k,
            min_score=min_score
        )


if __name__ == "__main__":
    # Test do query engine
    engine = CodeQueryEngine(collection_name="img_converter")
    
    # Teste 1: Query sem√¢ntica
    print("\n=== Teste 1: Query Sem√¢ntica ===")
    result = engine.query("Como funciona a compress√£o de PDF?")
    print(f"Tipo: {result['query_classification']['tipo']}")
    print(f"Resultados sem√¢nticos: {result['semantic_count']}")
    
    # Teste 2: Query de grafo
    print("\n=== Teste 2: Query de Grafo ===")
    result = engine.query("Quem chama compress_pdf?")
    print(f"Tipo: {result['query_classification']['tipo']}")
    print(f"Resultados de grafo: {result['graph_count']}")
