"""
Script de teste para demonstrar RAG completo com gera√ß√£o de resposta via LLM.
Fluxo: Recuperar contexto (RAG) ‚Üí Enviar para LLM (Ollama) ‚Üí Gerar resposta.
"""
import sys
from pathlib import Path

# Adiciona diret√≥rio pai ao path para importar m√≥dulos da aplica√ß√£o
sys.path.insert(0, str(Path(__file__).parent.parent))

# Carrega vari√°veis do arquivo .env
from dotenv import load_dotenv
load_dotenv()

from app.query_engine import CodeQueryEngine
import requests
import os
import time

# Configura√ß√µes do Ollama (carregadas do .env ou usa padr√£o)
OLLAMA_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5:3b")

def ask_ollama(context: str, question: str) -> str:
    """
    Envia contexto + pergunta para o Ollama e recebe resposta gerada.
    
    Args:
        context: Trechos de c√≥digo relevantes recuperados pelo RAG
        question: Pergunta/consulta do usu√°rio
        
    Returns:
        Resposta gerada pelo LLM (Ollama)
    """
    # Monta o prompt estruturado com contexto e pergunta
    prompt = f"""
Voc√™ √© um analista de c√≥digo s√™nior analisando um reposit√≥rio de c√≥digo.
Explique APENAS o que a fun√ß√£o faz, com base estrita no c√≥digo fornecido.
N√£o assuma comportamentos fora da fun√ß√£o.
N√£o generalize.
Se algo n√£o estiver claro no c√≥digo, diga explicitamente.

CONTEXTO (trechos relevantes do c√≥digo):
{context}

PERGUNTA:
{question}

Explique de forma clara e objetiva, em portugu√™s.
"""

    try:
        # Faz requisi√ß√£o HTTP para a API do Ollama
        print("‚è≥ Aguardando resposta do LLM (isso pode demorar)...")
        start_time = time.time()  # Registra tempo inicial
        
        response = requests.post(
            f"{OLLAMA_URL}/api/generate",  # Endpoint correto do Ollama para gerar texto
            json={
                "model": OLLAMA_MODEL,           # Modelo LLM a usar (ex: qwen3:1.7b)
                "prompt": prompt,                # Prompt estruturado com contexto
                "stream": False                  # False = aguarda resposta completa
            },
            timeout=300  # Aumentado para 300s (5 minutos) - LLM pode ser lento
        )

        response.raise_for_status()  # Lan√ßa exce√ß√£o se houver erro HTTP
        elapsed_time = time.time() - start_time  # Calcula tempo decorrido
        print(f"‚è±Ô∏è  Tempo de resposta: {elapsed_time:.2f}s\n")  # Exibe tempo
        return response.json()["response"]
    
    except requests.exceptions.Timeout:
        print("\n‚ùå Timeout: O Ollama demorou muito para responder.")
        print("üí° Sugest√µes:")
        print("   - Verifique se o Ollama est√° rodando: ollama serve")
        print("   - Use um modelo menor: qwen2.5:3b")
        print("   - Aumente o timeout ainda mais no c√≥digo")
        raise
    except requests.exceptions.ConnectionError:
        print("\n‚ùå Erro de conex√£o: N√£o conseguiu conectar ao Ollama em", OLLAMA_URL)
        print("üí° Inicie o Ollama: ollama serve")
        raise
    except Exception as e:
        print(f"\n‚ùå Erro ao comunicar com Ollama: {e}")
        raise


def main():
    """Fun√ß√£o principal: executa o pipeline RAG completo com LLM."""
    
    # Inicializa o engine com a cole√ß√£o indexada
    engine = CodeQueryEngine(collection_name="img_converter")

    # Pergunta que ser√° respondida com base no reposit√≥rio
    #question = "O que acontece quando compress_pdf() √© executado?"
    # for√ßa racioc√≠nio procedural...
    question = "Explique passo a passo a l√≥gica interna da fun√ß√£o compress_pdf(), incluindo crit√©rios de decis√£o e efeitos colaterais no sistema de arquivos."

    # Etapa 1: Recuperar contexto relevante (RAG - Retrieval)
    print("üîé Recuperando contexto do c√≥digo...\n")
    result = engine.query(question)

    # Formata os chunks recuperados em um texto estruturado
    # Inclui arquivo, score (relev√¢ncia) e conte√∫do
    context = "\n\n".join(
        f"Arquivo: {ctx['file_path']} (relev√¢ncia: {ctx['score']:.3f})\n{ctx['text']}"
        for ctx in result['context']
    )

    # Etapa 2: Enviar contexto + pergunta para o LLM (Augmentation + Generation)
    print("üß† Enviando contexto para o LLM...\n")
    answer = ask_ollama(context, question)

    # Etapa 3: Exibir resposta gerada
    print("‚úÖ Resposta final:\n")
    print(answer)


if __name__ == "__main__":
    main()

