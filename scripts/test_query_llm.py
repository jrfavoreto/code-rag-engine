"""
Script de teste para demonstrar RAG completo com gera√ß√£o de resposta via LLM.
Fluxo: Recuperar contexto (RAG) ‚Üí Enviar para LLM (Ollama) ‚Üí Gerar resposta.
"""
import sys
from pathlib import Path

# Adiciona diret√≥rio pai ao path para importar m√≥dulos da aplica√ß√£o
sys.path.insert(0, str(Path(__file__).parent.parent))

# Carrega vari√°veis do arquivo .env
from dotenv import load_dotenv
load_dotenv()

from app.query_engine import CodeQueryEngine
import requests
import os
import time

# Configura√ß√µes do Ollama (carregadas do .env ou usa padr√£o)
OLLAMA_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "qwen2.5:3b")

def ask_ollama(context: str, question: str) -> str:
    """
    Envia contexto + pergunta para o Ollama e recebe resposta gerada.
    
    Args:
        context: Trechos de c√≥digo relevantes recuperados pelo RAG
        question: Pergunta/consulta do usu√°rio
        
    Returns:
        Resposta gerada pelo LLM (Ollama)
    """
    # Monta o prompt estruturado com contexto e pergunta
    prompt = f"""
Voc√™ √© um analista de c√≥digo s√™nior analisando um reposit√≥rio de c√≥digo.
INSTRU√á√ïES CR√çTICAS:
1. Explique APENAS com base no c√≥digo fornecido
2. Cite trechos espec√≠ficos do c√≥digo quando afirmar algo
3. Se algo n√£o estiver expl√≠cito, diga "n√£o √© poss√≠vel afirmar"
4. N√£o inverta l√≥gicas (ex: "maior" vs "menor"). N√£o generalize.
5. Seja preciso em condicionais e compara√ß√µes
6. N√£o assuma comportamentos fora da fun√ß√£o.

CONTEXTO (trechos relevantes do c√≥digo):
{context}

PERGUNTA:
{question}

Explique de forma clara e objetiva, em portugu√™s.
"""

    try:
        # Faz requisi√ß√£o HTTP para a API do Ollama
        print("‚è≥ Aguardando resposta do LLM (isso pode demorar)...")
        start_time = time.time()  # Registra tempo inicial
        
        response = requests.post(
            f"{OLLAMA_URL}/api/generate",  # Endpoint correto do Ollama para gerar texto
            json={
                "model": OLLAMA_MODEL,           # Modelo LLM a usar (ex: qwen3:1.7b)
                "prompt": prompt,                # Prompt estruturado com contexto
                "stream": False                  # False = aguarda resposta completa
            },
            timeout=300  # Aumentado para 300s (5 minutos) - LLM pode ser lento
        )

        response.raise_for_status()  # Lan√ßa exce√ß√£o se houver erro HTTP
        elapsed_time = time.time() - start_time  # Calcula tempo decorrido
        print(f"‚è±Ô∏è  Tempo de resposta: {elapsed_time:.2f}s\n")  # Exibe tempo
        return response.json()["response"]
    
    except requests.exceptions.Timeout:
        print("\n‚ùå Timeout: O Ollama demorou muito para responder.")
        print("üí° Sugest√µes:")
        print("   - Verifique se o Ollama est√° rodando: ollama serve")
        print("   - Use um modelo menor: qwen2.5:3b")
        print("   - Aumente o timeout ainda mais no c√≥digo")
        raise
    except requests.exceptions.ConnectionError:
        print("\n‚ùå Erro de conex√£o: N√£o conseguiu conectar ao Ollama em", OLLAMA_URL)
        print("üí° Inicie o Ollama: ollama serve")
        raise
    except Exception as e:
        print(f"\n‚ùå Erro ao comunicar com Ollama: {e}")
        raise


def main():
    """Fun√ß√£o principal: executa o pipeline RAG completo com LLM."""
    
    # Inicializa o engine com a cole√ß√£o indexada
    print("üöÄ Inicializando Code RAG Engine...")
    engine = CodeQueryEngine(collection_name="img_converter")
    print()

    # Recebe a pergunta do usu√°rio via terminal
    print("=" * 70)
    print("üí¨ Digite sua pergunta sobre o c√≥digo (ou 'sair' para encerrar)")
    print("=" * 70)
    
    while True:
        question = input("\n‚ùì Pergunta: ").strip()
        
        # Verifica se o usu√°rio quer sair
        if question.lower() in ['sair', 'exit', 'quit', 'q']:
            print("\nüëã Encerrando...")
            break
        
        # Valida se a pergunta n√£o est√° vazia
        if not question:
            print("‚ö†Ô∏è  Por favor, digite uma pergunta v√°lida.")
            continue
        
        print()  # Linha em branco para separar

        # Etapa 1: Recuperar contexto relevante (RAG - Retrieval)
        print("üîé Recuperando contexto do c√≥digo...\n")
        result = engine.query(question)

        # Formata os chunks recuperados em um texto estruturado
        # Inclui arquivo, score (relev√¢ncia) e conte√∫do
        context = "\n\n".join(
            f"Arquivo: {ctx['file_path']} (relev√¢ncia: {ctx['score']:.3f})\n{ctx['text']}"
            for ctx in result['context']
        )

        # Etapa 2: Enviar contexto + pergunta para o LLM (Augmentation + Generation)
        print("üß† Enviando contexto para o LLM...\n")
        answer = ask_ollama(context, question)

        # Etapa 3: Exibir resposta gerada
        print("‚úÖ Resposta:\n")
        print(answer)
        print("\n" + "=" * 70)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nüëã Interrompido pelo usu√°rio. At√© logo!")
    except Exception as e:
        print(f"\n‚ùå Erro fatal: {e}")
        sys.exit(1)

